{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34d0cc5c",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3233f204",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5142683e",
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\"\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "    \"loftq.pdf\",\n",
    "    \"swebench.pdf\",\n",
    "    \"selfrag.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3b1fdab",
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: selfrag.pdf\n",
      "Getting tools for paper: loftq.pdf\n",
      "Getting tools for paper: swebench.pdf\n",
      "Getting tools for paper: selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d08c76ee",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3734253",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c327184f",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1cda585d",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc3e095a",
   "metadata": {
    "height": 29
   },
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0572276f",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9397336a",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Useful for summarization questions related to selfrag', name='summary_tool_selfrag', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9552156c",
   "metadata": {
    "height": 266
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0652df40",
   "metadata": {
    "height": 29
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation dataset used in LongLoRA\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in LongLoRA includes the book corpus dataset PG19, the cleaned Arxiv Math proof-pile dataset, the PG19 validation set from the Redpajama dataset, the PG19 test split, LongAlpaca-12k containing long-context and short question-answer pairs from the original Alpaca data, and the dataset called SAFECONV.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results consistently demonstrate the effectiveness and efficiency of the proposed methods in extending the context window for Llama2 models, achieving comparable or better performance compared to full attention or full fine-tuning baselines. The models show improved perplexity with longer context sizes and perform well on various tasks, showcasing state-of-the-art performance on benchmarks and tasks like topic retrieval. Different attention patterns have varying impacts on model performance during fine-tuning, with dilated attention performing well in full fine-tuning scenarios and the proposed methods bridging the performance gap between low-rank adaptation and full fine-tuning. The models also outperform other long-context models like Vicuna and LongChat, demonstrating their effectiveness and generalization capabilities across datasets and manipulations.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA includes various datasets such as the book corpus dataset PG19, the cleaned Arxiv Math proof-pile dataset, the PG19 validation set from the Redpajama dataset, the PG19 test split, LongAlpaca-12k, and the SAFECONV dataset. \n",
      "\n",
      "The evaluation results of LongLoRA show that the proposed methods effectively extend the context window for Llama2 models, achieving comparable or better performance compared to full attention or full fine-tuning baselines. The models demonstrate improved perplexity with longer context sizes and perform well on various tasks, including topic retrieval. Different attention patterns have varying impacts on model performance during fine-tuning, with dilated attention performing well in full fine-tuning scenarios. The models also outperform other long-context models like Vicuna and LongChat, showcasing their effectiveness and generalization capabilities across datasets and manipulations.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9177eca5",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and SoftwareDev.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in SWE-Bench consists of software engineering task instances collected from real GitHub repositories, including issues and corresponding pull requests. It includes task instructions, issue text, retrieved files and documentation, example patch files, and prompts for generating patch files. The dataset is constructed by scraping pull requests from popular Python repositories, ensuring that the selected PRs are merged, resolve issues, and introduce new tests. Task instances are validated through execution-based validation to ensure usability, with a focus on creating executable contexts, checking solutions and tests, and examining execution logs. The dataset covers a wide range of challenges in software engineering, such as patch generation, reasoning over long contexts, navigating codebase directories, and capturing dependency-based relationships across modules. Task instances are categorized by tags like Bug, Feature, Regression, and Other, showcasing a diverse set of code changes for evaluation.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and SoftwareDev. \n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench consists of software engineering task instances collected from real GitHub repositories, including issues and corresponding pull requests. It includes task instructions, issue text, retrieved files and documentation, example patch files, and prompts for generating patch files. The dataset is constructed by scraping pull requests from popular Python repositories, ensuring that the selected PRs are merged, resolve issues, and introduce new tests. Task instances are validated through execution-based validation to ensure usability, with a focus on creating executable contexts, checking solutions and tests, and examining execution logs. The dataset covers a wide range of challenges in software engineering, such as patch generation, reasoning over long contexts, navigating codebase directories, and capturing dependency-based relationships across modules. Task instances are categorized by tags like Bug, Feature, Regression, and Other, showcasing a diverse set of code changes for evaluation.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
